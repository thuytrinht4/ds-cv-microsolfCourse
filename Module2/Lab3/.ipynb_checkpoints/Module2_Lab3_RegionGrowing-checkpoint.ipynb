{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Module 2, Lab 3, \"Region Growing\" and \"Superpixels\"\n",
    "\n",
    "\n",
    "# Recap\n",
    "\n",
    "This is the third of four labs for Lesson 2 - Classical Image Segmention in Dev290x. You should complete the tasks in this lab as part of the Region Growing section of the lesson.\n",
    "\n",
    "Please remember this lab must be completed before taking the quiz at the end of this lesson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab you will complete your third image segmentation project where you will use the watershed algorithm to segment a relatively complex image containing objects which are touching each other. \n",
    "\n",
    "You will also complete a short super-pixel based segmentation example.\n",
    "\n",
    "Please work through these projects using the image processing techniques from the previous labs and then segment the example images, once using watershed and once using super-pixels.\n",
    "\n",
    "At the end of the lab we'll review the work we've done and assess what types of images and projects these approaches are effective for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "In this lab, you will: \n",
    "* learn about one introductary 'region growing' image segmentation techique - 'watershed';\n",
    "* revise deblurring, thresholding, dilation, opening, and the distance transformation;\n",
    "* use cv2.watershed() to segment an image;\n",
    "* gain an insight into superpixels.  This is a very important topic in neural network based image processing!\n",
    "* segment the tomatoes image into superpixels\n",
    "* use the SLIC algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Image segmentation is the process of partitioning a digital image into multiple segments to make the image easier to analyze.  Often we are looking to locate objects and boundaries in the original image.  Another way of looking at it is image segmentation's goal is to assign a label to every pixel in an image such that pixels with the same label share certain characteristics.  Like many elements of computer vision, I find an example is often more useful than precise text.\n",
    "\n",
    "For example, these images show a typical road scene on the left and a segmented version of the image on the right.\n",
    "\n",
    "<p float=\"center\">\n",
    "  <img src=\"../Images/bg-road.png\" width=\"450\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region Growing - 'Watershed'\n",
    "\n",
    "The simplest image segmentation techniques, like the thresholding we used in Lab 1, start to show their limitations pretty quickly - requirements like segmenting beyond two objects, touching objects or overlapping objects, etc quickly lead us to techniques such as Watershed,\n",
    "\n",
    "'Watershed' is a (mostly) unsuperised algorithm used to isolate portions of an image from each other - in\n",
    "effect image segmentation.  Its worth noting, however, that - similar to most classical image processing 'watershed' works by identifying elements of the image such as color intensity. i.e. 'watershed' has no semantic understanding of the contents of the image.\n",
    "\n",
    "The watershed algorithm is useful for segmentation and is relatively strong at detecting touching or overlapping objects in images such as the image of tomatoes below.\n",
    "\n",
    "<img src=\"../Images/tomatoes.png\" alt=\"Tomatoes\" style=\"width: 200px;\"/>\n",
    "\n",
    "So to summarise, algorithms like 'watershed' have applications where simpler methods such as thresholding and contour detection fail. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory behind Watershed\n",
    "\n",
    "After converting an image to greyscale, we can imagine that greyscale image as being a topographic surface (a height map) where high intensity denotes 'hills' and low intensity denotes 'valleys'.  Conceptually, we start by assigning every isolated 'valley' with different colour liquid.  These correspond to our labels.  As the liquids rise, the various liquids from different valleys start to merge.  We want to avoid these mergers.  To avoid mergers, we build barriers at the merger locations.  We continue to fill the liquids until all the hills are under liquid.  \n",
    "\n",
    "We can visually grasp the concept using the simple square image on the left and its topographic representation on the right.  We can imagine filling the topographic representation of our square with two liquids, one inside the square and one outside the square.  As each liquid meets each other we create a 'watershed', and we can imagine this representing the square.\n",
    "\n",
    "Original                 | Topography            \n",
    ":-----------------------:|:--------------------------------:\n",
    "<img src=\"../Images/simple_square.png\" alt=\"Simple Square\" style=\"width: 200px;\"/> | <img src=\"../Images/simple_square_topography.png\" alt=\"Simple Square Topography\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "Similarly, we can imagine the tomatoes below on the left, being transformed into a 3d shape being filled with a liquid until the individual tomatoes emerge.\n",
    "\n",
    "Original                 | Topography            \n",
    ":-----------------------:|:--------------------------------:\n",
    "<img src=\"../Images/tomatoes.png\" alt=\"Tomatoes\" style=\"width: 200px;\"/> | <img src=\"../Images/tomatoes_topography.png\" alt=\"Tomatoes Topography\" style=\"width: 200px;\"/>\n",
    "\n",
    "## Watershed in Practice\n",
    "\n",
    "The watershed algorithm tends to give us over-segmented results due to noise etc in the image.  So its typical to simplify the image before sending it to the Watershed algorithm, and as you'll see below, there's almost invariably a denoising step.\n",
    "\n",
    "\n",
    "## OpenCV Watershed\n",
    "\n",
    "A big improvement of the watershed transformation consists of flooding the topographic surface from a previously defined set of markers. This effectively automates the process of starting the watershed algorithm off by giving\n",
    "it hints where to start filling from.\n",
    "\n",
    "The OpenCV watershed algorithm has two parameters, the image being worked on and a second image.\n",
    "This second image contains 'markers'. These markers must be 'user-defined'.  \n",
    "\n",
    "We won't manually define them (e.g. using point-and-click; instead we'll heuristically define them instead using thresholding and/or morphological operations.\n",
    "\n",
    "Then we'll apply the watershed algorithm to our image using that marker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Technique\n",
    "In this technique, we will see how to use the Distance Transformation along with the watershed algorithm to segment some objects.\n",
    "\n",
    "Let''s remind ourselves of our image of tomatoes.  As you can see, this is an image where several of the objects are touching or overlapping.  We could tune the thresholding algorithm we used in Lab 1 or the clustering algorithm we used in Lab 2 to detect multiple objects and segment the image into those objects but its likely that they will be unable to distinguish between the two red tomatoes on the left hand side of the image as we look at it.\n",
    "\n",
    "<img src=\"../Images/tomatoes.png\" alt=\"Tomatoes\" style=\"width: 300px;\"/>\n",
    "\n",
    "Instead, we'll start by preparing the image such that the OpenCV 'waterfall' can operate on this image.  \n",
    "\n",
    "Our overall goal here is to construct a 'marker' image for OpenCV's 'waterfall' routine.  The marker area is, effectively, an initial area of the image which guides the 'waterfall' algorithm to help with over-segmenting.\n",
    "\n",
    "Initially we will create an object mask, a background mask and then subtract one from the other to create a mask of the 'unknown' part of the image.\n",
    "\n",
    " So, in this example, we will 'semi-automate' the process of providing marker to watershed algo\n",
    " 1. decide what is clearly background\n",
    " 2. decide what is clearly foreground\n",
    " 3. create a region that's neither clearly background nor foreground\n",
    " 4. set up 'marker' around this area\n",
    " 5. hand that off to watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by converting the tomatoes image to greyscale, applying a little Gaussian blur and then using a relatively high threshold to to simplify the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to previous labs, we're going to use OpenCV, NumPY and MatPlotLib\n",
    "# so import them here\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load an image\n",
    "img = cv2.imread(\"../Images/tomatoes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# The degree of blurring is strongly coupled with the performance of the watershed.  \n",
    "# It can be quite interesting to experiment with adjusting the amount of blurring.\n",
    "# Exercise: Try using OpenCV2's GaussianBlur until your image is satisfactory.\n",
    "# You have a reference image below to help you if you want it.\n",
    "# Hint: You'll find answers at the bottom of the lab. \n",
    "\n",
    "# END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to grayscale so we can threshold it\n",
    "gray = cv2.cvtColor(blur, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.imshow(gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after these operations, you should be starting to see the tomatoes emerge from the background.  \n",
    "\n",
    "Your blurred grayscale image should be similar to the image here.\n",
    "\n",
    "<img src=\"../Images/tomatoes_grayscale.png\" alt=\"Segmented Starfish\" align=\"left\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Exercise: So, create a matrix thresh which is thresholded by OpenCV's threshold() operation.\n",
    "# HINT: I did not using OTSU here as I got better results from \n",
    "# weighting the threshold high. You can experiment here with thresholding to \n",
    "# improve on the final segmentation.\n",
    "#\n",
    "# Use the method cv2.threshold() which returns two values. The second value is the\n",
    "# value you are interested in, you can ignore the first for now...\n",
    "#\n",
    "# Store the value in a variable called threshold\n",
    "# Hint: You'll find answers at the bottom of the lab. \n",
    "\n",
    "# END YOUR CODE GOES HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(thresh, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should have a matrix (or image if you prefer) which is similar to this image.\n",
    "\n",
    "After these operations, we're starting to see the tomatoes emerge from the background.  Note in this example that, its a little more complex than the starfish example, in that the objects of interest are touching and/or overlapping.\n",
    "\n",
    "<img src=\"../Images/tomatoes_thresh.png\" alt=\"Tomatoes B&W\" align=\"left\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the marker image\n",
    "\n",
    "Now you need to prepare the foreground, background, and unknown regions of the image to create a marker for OpenCV's watershed().\n",
    "\n",
    "At this stage, we can start preparing the foreground, background, and 'unknown' regions of the image.  For this image, I used a closing morphological operation followed by a dilation to generate the background image and I used a distance transform to generate the foreground image.  Finally, I subtracted those two images from each other to generate the unknown area of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Close the image a little to fill in a few small holes in it\n",
    "closingKernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "\n",
    "# Create a matrix closed that is generated from thresh by a closing\n",
    "# operation using the kernel above.\n",
    "closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, closingKernel)\n",
    "\n",
    "# Use just enough dilate to get some clearly identifiable background\n",
    "dilationKernel = np.ones((3,3), np.uint8)\n",
    "\n",
    "# create a matrix 'bg' from OpenCV's dilate() function\n",
    "# using the dilation kernel above\n",
    "bg = cv2.dilate(closed, dilationKernel, iterations=3)\n",
    "\n",
    "# Now use a distance transform to extract is clearly foreground\n",
    "\n",
    "# Create a matrix 'dist_transform' using OpenCV's distanceTransform\n",
    "# method on the 'closed' matrix.\n",
    "dist_transform = cv2.distanceTransform(closed, cv2.DIST_L2, 5)\n",
    "\n",
    "# Threshold the distance transformation\n",
    "ret, fg = cv2.threshold(dist_transform,0.7*dist_transform.max(), 255, 0)\n",
    "\n",
    "# Now find the unknown region by subtracting one from the other\n",
    "fg = np.uint8(fg)\n",
    "unknown = cv2.subtract(bg, fg)\n",
    "\n",
    "plt.imshow(unknown, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point your unknown region should be substantially similar to the image on the right here.  I've also included a reference background image and foreground (or object) image in case you lose your way a little.\n",
    "\n",
    "Background                 | Foreground         | Unknown            \n",
    ":-----------------------:|:--------------------------------:|--------------:\n",
    "<img src=\"../Images/tomatoes_bg.png\" alt=\"Tomatoes BG\" style=\"width: 200px;\"/> | <img src=\"../Images/tomatoes_fg.png\" alt=\"Tomatoes FG\" style=\"width: 200px;\"/> | <img src=\"../Images/tomatoes_unknown.png\" alt=\"Tomatoes Unknown\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to do a little data manipulation before calling OpenCV's watershed. \n",
    "\n",
    "Right now what we have in marker is a set of pixel values, with 0 representing the background and 255 representing the unknown region and what OpenCV's watershed expects to see in marker is the unknown region set to 0. So, you need to run the following code to prepare for watershed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marker labelling\n",
    "ret, marker = cv2.connectedComponents(fg)\n",
    "\n",
    "# Add one to all labels so that bg is not  0, but 1\n",
    "marker = marker+1\n",
    "\n",
    "# Now, mark the region of unknown with 0\n",
    "marker[unknown==255] = 0;\n",
    "\n",
    "plt.imshow(marker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage you should be seeing an image substantively similar to one shown here.\n",
    "\n",
    "<img src=\"../Images/tomatoes_marker.png\" alt=\"Tomatoes Marker\" align=\"left\" style=\"width: 300px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run watershed over our image. Then we'll colour the segmentation boundaries the watershed found in green and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now marker is ready.  It is time for last step\n",
    "cv2.watershed(img, marker)\n",
    "\n",
    "# Create a new empty image with the same shape\n",
    "# as the original image.\n",
    "h, w, num_c = img.shape\n",
    "seg = np.zeros((h, w, num_c), np.uint8)\n",
    "\n",
    "# Watershed has replaced the pixel\n",
    "# values in marker with integers representing\n",
    "# the segments it has found in the original\n",
    "# image.\n",
    "# Color in these segments\n",
    "# \n",
    "maxMarker = np.max(marker)\n",
    "minMarker = np.min(marker)\n",
    "\n",
    "colorMap =  [ \\\n",
    "             [0,0,0], \\\n",
    "             [255,255,255], \\\n",
    "             [127,0,0], \\\n",
    "             [0,0,255], \\\n",
    "             [0,255,0], \\\n",
    "             [255,0,0], \\\n",
    "             [255,255,0], \\\n",
    "             [255,0,255], \\\n",
    "             [0,255,255] \\\n",
    "            ]\n",
    "\n",
    "for region in range(minMarker, maxMarker+1):\n",
    "    seg[marker==region] = colorMap[region+1]\n",
    "    \n",
    "plt.imshow(cv2.cvtColor(seg, cv2.COLOR_BGR2RGB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your image should be similar to the one I've shown here.\n",
    "\n",
    "<img src=\"../Images/tomatoes_segmented.png\" alt=\"Tomatoes Segmented\" align=\"left\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few items I want to highlight at this stage - most notably we've missed the yellow tomato on top t the right and two of our tomatoes have 'merged'.  \n",
    "\n",
    "We can continue to tweak to improve the segmentation for this image, but its probably more productive to just note, for now, that - with classical algorithms - we can often end up hand-crafting our features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIC\n",
    "\n",
    "We'll be talking about super-pixels at a later stage in this module. We're going to do a short lab with super-pixels here to introduce them.\n",
    "\n",
    "You'll find a more detailed discussion of super-pixels and the SLIC algorithm in the accompanying lesson for this lab.\n",
    "\n",
    "In summary, an isolated pixel does conveys extremely little information about the image it resided within. There would appear to be good benefits from gathering pixels together to gain some semantic information - i.e. to create a 'thing' which is capable of carrying some 'meaning'.\n",
    "\n",
    "If we call out the work of Dr. Xiaofeng Ren's work here super-pizels would ideally contain perceptual meaningfulness - for instance colour, texture, etc (and ideally semantic value). This property leads towards their use in neural networks.\n",
    "\n",
    "As you will see, super-pixels also offer computational efficiency - the reduce the complexity of images from hundreds of thoushands of pixels to hundreds of pixels.\n",
    "\n",
    "Super-pixelation tends to lead to the detection of the boundaries we desire while having a side-effect of over-segmenting and introducing boundaries we don't want.  Pleae keep this insight as you work through this lab and retain it as you work through later lessons.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this algorithm, you'll need to import the 'segmentation' package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation, color\n",
    "from skimage.io import imread\n",
    "from skimage.io import imsave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load your image.  We''re going to use the relatively complex tomatoes image again.  However, this time, please don''t blur or grayscale the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "img = imread(\"../Images/tomatoes.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the SLIC super-pixel algorithm over the image, varying the compactness and the numsegments until the image you produce resembles the image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Exerise: Vary n_segments to find a reasonable value for it for this image\n",
    "# Important Note: You are manually selecting features and feature properties \n",
    "# (e.g. size of super-pixels) here\n",
    "# Hint: You'll find answers at the bottom of the lab. \n",
    "\n",
    "# END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compactFactor = 20\n",
    "\n",
    "img_segments = segmentation.slic(img, compactness=compactFactor, n_segments=numSegments)\n",
    "superpixels = color.label2rgb(img_segments, img, kind='avg')\n",
    "\n",
    "plt.imshow(superpixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your image should be similar to the one shown here.\n",
    "\n",
    "<img src=\"../Images/tomatoes_slic.png\" alt=\"Tomatoes Superpixel version\" align=\"left\" style=\"width: 300px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the assignment, you should run the watershed algorithm over the image you created in the last step.\n",
    "\n",
    "Your aim is to produce an image similar to the one below.\n",
    "\n",
    "<img src=\"../Images/tomatoes_slic_watershed_seg.png\" alt=\"Tomatoes Superpixel Watershed Segmented\" align=\"left\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "So, that completes the third of the four labs in this lesson.\n",
    "\n",
    "You've learned how to use erosion and dilation\n",
    "You've learned how to use the watershed algorithm to segment an image with touching or overlapping regions.\n",
    "You've been introduced to super-pixels and the SLIC algorithm\n",
    "You've gained an insight that classical techniques involve manually tuning features like super-pixel size for individual images or classes of images and you should be asking yourself \"surely, there's a better way?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Degree of Blurring Missing Code:**\n",
    "\n",
    "    blur = cv2.GaussianBlur(img, (21,21), 0)\n",
    "\n",
    "**Solution to Thresholding Missing Code:**\n",
    "\n",
    "    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "**Solution for the SLIC Missing Code:**\n",
    "    \n",
    "    numSegments = 800\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

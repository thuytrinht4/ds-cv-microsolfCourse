{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3, Lab 2: Object Classification - ResNet\n",
    "\n",
    "In this tutorial, we will gain some practical experience using Microsoft ResNet, a Deep Convolutional Neural Network (CNN) approach to object classification. Convolutional Neural Networks build up layers of convolutions, transforming an input image and distilling it down until they start recognizing composite features, with deeper layers of convolutions recognizing increasingly complex patterns. \n",
    "\n",
    "As you will have seen from our accompanying video lectures in this module, ResNet was invented by Researchers at Microsoft Research Asia (Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun), and it won first place in both ILSVRC and MS COCO competitions in  2015. \n",
    "\n",
    "A great visual presentation from Kaiming He describing ResNet is available at:\n",
    "\n",
    "* http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf. \n",
    "\n",
    "Whereas AlexNet (ILSVRC 2012 winner) had 8 layers, VGG (ILSRVC 2014) had 19 layers, and GoogleNet (ILSRVC 2014) had 22 layers, ResNet demonstrated a trainable network with a whopping 152 layers! This was quite a technical achievement at the time. ResNet managed this by resolving a phenomenon that had to this point been observed across many different datasets - namely, that deeper networks experienced higher training and test error than their shallower counterparts.\n",
    "\n",
    "By contrast, deep ResNets can be easily trained, and in fact have lower training error and lower test error than comparable, shallower, networks.   Furthermore, ResNets can learn better features which are more transferrable to networks used for other tasks.  These better features lead to improved performance on object detection and segmentation tasks when using ResNet as the base classifier.\n",
    "\n",
    "ResNets achieve this through the use of a technique called *Residual Deep Learning*. This is a technique that originated in Microsoft Research and involves *passing through* the main signal of the input data, so that the network ultimately \"learns\" on just the residual portions that differ between layers. This has proven, in practice, to allow the training of much deeper networks by avoiding issues that plague gradient descent on larger networks. These cells bypass convolution layers and then come back in later before rectified linear unit (ReLU).\n",
    "\n",
    "For this task, we have chosen ResNet20 as our trained model architecture. In our next module, this ResNet20 model will be used as a base model, and we will build upon it to introduce concepts such as *Transfer Learning* for classification, and *Semantic Segmentation*.\n",
    "\n",
    "\n",
    "# Setup Code\n",
    "\n",
    "First, we need  to load the Microsoft Cognitive Toolkit (previously known as the CNTK) libraries and other dependencies for this exerise.\n",
    "\n",
    "The Microsoft Cognitive Toolkit allows you to harness the intelligence within massive datasets through deep learning by providing uncompromised scaling, speed, and accuracy with commercial-grade quality and compatibility with the programming languages and algorithms you already use. \n",
    "\n",
    "The toolkit trains and evaluates deep learning algorithms faster than other available toolkits, scaling efficiently in a range of environments—from a CPU, to GPUs, to multiple machines—while maintaining accuracy.\n",
    "\n",
    "It is built with sophisticated algorithms and production readers to work reliably with massive datasets. Skype, Cortana, Bing, Xbox, and industry-leading data scientists already use the Microsoft Cognitive Toolkit to develop commercial-grade AI.\n",
    "\n",
    "The toolkit offers the most expressive, easy-to-use architecture available. Working with the languages and networks you know, like C++ and Python, it empowers you to customize any of the built-in training algorithms, or use your own.\n",
    "\n",
    "We need version 2.4 of the Microsoft Cognitive Toolkit, so we'll use the Python package manager *pip* to upgrade if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "#\n",
    "# Licensed under the MIT license. See LICENSE.md file in the project root\n",
    "# for full license information.\n",
    "# ==============================================================================\n",
    "\n",
    "# For Azure Notebooks, we will update Microsoft Cognitive Toolkit version to 2.4 \n",
    "# you can comment out the following line if you are running in your own local Jupyter Notebook setup and already have\n",
    "# CNTK 2.4 installed\n",
    "!pip install --upgrade --no-deps https://cntk.ai/PythonWheel/CPU-Only/cntk-2.4-cp35-cp35m-linux_x86_64.whl\n",
    "\n",
    "import cntk as C\n",
    "print (\"Using Microsoft Cognitive Toolkit version {}\".format(C.__version__))\n",
    "\n",
    "import numpy as np\n",
    "print (\"Using numpy version {}\".format(np.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the rest of the Python routines we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "from IPython.display import Image, display, SVG\n",
    "import time\n",
    "import seaborn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "# ==============================================================================\n",
    "# Import CNTK and helpers\n",
    "#\n",
    "from cntk import load_model\n",
    "from cntk.initializer import he_normal, normal\n",
    "from cntk.layers import AveragePooling, MaxPooling, BatchNormalization, Convolution, Dense\n",
    "from cntk.ops import element_times, relu\n",
    "#\n",
    "# For Training\n",
    "from cntk import cross_entropy_with_softmax, classification_error, reduce_mean\n",
    "from cntk import Trainer, cntk_py\n",
    "from cntk.io import MinibatchSource, ImageDeserializer, StreamDef, StreamDefs\n",
    "from cntk.learners import momentum_sgd, momentum_schedule,  learning_parameter_schedule_per_sample\n",
    "from cntk.debugging import *\n",
    "from cntk.logging import *\n",
    "import cntk.io.transforms as xforms\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#\n",
    "# Paths relative to current python file.\n",
    "abs_path   = os.path.dirname(\".\")\n",
    "data_path  = os.path.join(\"/home/nbuser/data/M3\", \"CIFAR-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Cognitive Toolkit's default policy to use the best available device (GPU, if available, else CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    isUsingGPU # if this is our first time running, this will cause an exception as undefined\n",
    "except NameError:\n",
    "    try:\n",
    "        isUsingGPU = C.device.try_set_default_device(C.device.gpu(0))\n",
    "    except ValueError:\n",
    "        isUsingGPU = False\n",
    "        C.device.try_set_default_device(C.device.cpu())\n",
    "\n",
    "print (\"The Cognitive Toolkit is using the {} for processing.\".format(\"GPU\" if isUsingGPU else \"CPU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Building Blocks of CNN\n",
    "\n",
    "*Convolutional Neural Networks* (also known as *CNNs*, or *ConvNets*) typically stack convolutional layers, rectified linear unit (ReLU) activation layers, pooling layers, and finally fully connected layers. \n",
    "\n",
    "ResNet changes this slightly, and introduces a new trend towards smaller filters and deeper architectures, with fewer pooling and fully connected layers.\n",
    "\n",
    "We need to create some basic building blocks for the ResNet model. Let's take a look at the Resnet *residual block*, which we will use to create our the full ResNet20 network architecture.\n",
    "\n",
    "![The Residual Block](images/residual_block_color.svg \"The Residual Block\")\n",
    "\n",
    "As you can see, the residual block consists of a stacking of convolutional, batch normalization, and rectified linear (ReLU) activation layers. This residual block is our basic building block for ResNet.  We explained how it works in our video lectures for this module, and will recap on this shortly.  \n",
    "\n",
    "To start with, we're going to create two functions to help us stack convolutional layers with batch normalization layers, and then convolutional layers with batch normalization and ReLU layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# assembly components\n",
    "#\n",
    "# We're using a stride of (1,1) for the convolution - i.e. we stride (move) by 1 pixel horizontally and 1 pixel vertically...\n",
    "#\n",
    "# We're using he_hormal() for our initializer - this will set parameters to Gaussian distribution with mean \n",
    "#  0 and standard deviation of scale∗sqrt(2.0/fanIn)\n",
    "#\n",
    "# Batch normalization applies this formula to every input element (element-wise): \n",
    "#   y = (x - batch_mean) / (batch_stddev + epsilon) * scale + bias \n",
    "# where batch_mean and batch_stddev are estimated on the minibatch, and scale and bias are learned parameters.\n",
    "#\n",
    "# \n",
    "def conv_bn(input, filter_size, num_filters, strides=(1, 1), init=he_normal(), bn_init_scale=1):\n",
    "    c = Convolution(filter_size, num_filters, activation=None, init=init, pad=True, strides=strides, bias=False)(input)\n",
    "    r = BatchNormalization(map_rank=1, normalization_time_constant=4096, use_cntk_engine=False, init_scale=bn_init_scale, disable_regularization=True)(c)\n",
    "    return r\n",
    "\n",
    "def conv_bn_relu(input, filter_size, num_filters, strides=(1, 1), init=he_normal()):\n",
    "    r = conv_bn(input, filter_size, num_filters, strides, init, 1)\n",
    "    return relu(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have a function to create a convolutional weights layer with back normalization at the end, and one which also has a rectified linear unit (ReLU) after it.  We're next going to combine both of these into the fundamental building block of ResNet, the *Residual Block*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Residual Block and Skip Connections\n",
    "\n",
    "The following two functions will create the residual block, and a version of the residual block that also adds in input passed through a single convolutional layer and batch norm.\n",
    "\n",
    "![resnet_basic_blocks](images/resnet_basic_blocks.svg \"ResNet basic building blocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ResNet components\n",
    "#\n",
    "def resnet_basic(input, num_filters):\n",
    "    c1 = conv_bn_relu(input, (3, 3), num_filters)\n",
    "    c2 = conv_bn(c1, (3, 3), num_filters, bn_init_scale=1)\n",
    "    p = c2 + input\n",
    "    return relu(p)\n",
    "\n",
    "def resnet_basic_inc(input, num_filters, strides=(2, 2)):\n",
    "    c1 = conv_bn_relu(input, (3, 3), num_filters, strides)\n",
    "    c2 = conv_bn(c1, (3, 3), num_filters, bn_init_scale=1)\n",
    "    s = conv_bn(input, (1, 1), num_filters, strides) # Shortcut\n",
    "    p = c2 + s\n",
    "    return relu(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the skip connection implemented above with the addition of c2 plus either the input layer or s, i.e.: \n",
    "\n",
    "$ p = c2 + input$\n",
    "\n",
    "or\n",
    "\n",
    "$p = c2 + s$\n",
    "\n",
    "\n",
    "Now we need to create some code to stack layers together, increasing the number of filters and decreasing the spatial size of the data as it progresses through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resnet_basic_stack(input, num_stack_layers, num_filters): \n",
    "    assert(num_stack_layers >= 0)\n",
    "    l = input\n",
    "    for _ in range(num_stack_layers):\n",
    "        l = resnet_basic(l, num_filters)\n",
    "    return l\n",
    "\n",
    "def resnet_bottleneck(input, out_num_filters, inter_out_num_filters):\n",
    "    c1 = conv_bn_relu(input, (1, 1), inter_out_num_filters)\n",
    "    c2 = conv_bn_relu(c1, (3, 3), inter_out_num_filters)\n",
    "    c3 = conv_bn(c2, (1, 1), out_num_filters, bn_init_scale=0)\n",
    "    p = c3 + input\n",
    "    return relu(p)\n",
    "\n",
    "def resnet_bottleneck_inc(input, out_num_filters, inter_out_num_filters, stride1x1, stride3x3):\n",
    "    c1 = conv_bn_relu(input, (1, 1), inter_out_num_filters, strides=stride1x1)\n",
    "    c2 = conv_bn_relu(c1, (3, 3), inter_out_num_filters, strides=stride3x3)\n",
    "    c3 = conv_bn(c2, (1, 1), out_num_filters, bn_init_scale=0)\n",
    "    stride = np.multiply(stride1x1, stride3x3)\n",
    "    s = conv_bn(input, (1, 1), out_num_filters, strides=stride) # Shortcut\n",
    "    p = c3 + s\n",
    "    return relu(p)\n",
    "\n",
    "def resnet_bottleneck_stack(input, num_stack_layers, out_num_filters, inter_out_num_filters): \n",
    "    assert(num_stack_layers >= 0)\n",
    "    l = input\n",
    "    for _ in range(num_stack_layers):\n",
    "        l = resnet_bottleneck(l, out_num_filters, inter_out_num_filters)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet20 Network Architecture\n",
    "\n",
    "With these functions, we now have enough to plumb together the architecture of ResNet20.  We are using the network architecture of ResNet defined in the [ResNet paper](http://arxiv.org/abs/1512.03385).\n",
    " \n",
    "The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. [Batch normalization](https://arxiv.org/abs/1502.03167) is applied everywhere except the last fully-connected layer.\n",
    "\n",
    "For now, let's look at how we stack our building blocks above blocks together to form the ResNet20 model.  At the beginning, we have a single convolutional weights layer, followed by a batch normalization layer and a rectified linear unit.  Then we have our residual units, and we end with an average pooling layer and a fully-connected (also called a *dense*) layer which gives us our class predictions.\n",
    "\n",
    "In the deeper ResNet Models, bottleneck blocks are introduced for computational considerations - i.e., they serve to reduce the dimensionality of the network, which will help prevent overfitting and improve inference and training time. In all ResNet variants, dimensionality is reduced at the end by the global average pooling layer. This layer further helps minimize the chances of overfitting by reducing the number of total parameters within the model (i.e., reducing the spatial dimensions of 3D tensors). See https://arxiv.org/pdf/1312.4400.pdf (especially section 3.2) for more details on global average pooling. ResNet doesn't require the use of dropout because it uses global average pooling instead to reduce the dimensionality before the dense fully-connected layer.\n",
    "\n",
    "ResNet20 doesn't use bottlenecks, so we won't discuss this further in this lab.\n",
    "\n",
    "Here is the full ResNet20 network architecture:\n",
    "\n",
    "![ResNet20 model](images/resnet20_model_color.svg \"ResNet20 Network Architecture\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Defines the residual network model for classifying images\n",
    "#\n",
    "def create_cifar10_model(input, num_stack_layers, num_classes):\n",
    "    c_map = [16, 32, 64]\n",
    "\n",
    "    conv = conv_bn_relu(input, (3, 3), c_map[0])\n",
    "    r1 = resnet_basic_stack(conv, num_stack_layers, c_map[0])\n",
    "\n",
    "    r2_1 = resnet_basic_inc(r1, c_map[1])\n",
    "    r2_2 = resnet_basic_stack(r2_1, num_stack_layers-1, c_map[1])\n",
    "\n",
    "    r3_1 = resnet_basic_inc(r2_2, c_map[2])\n",
    "    r3_2 = resnet_basic_stack(r3_1, num_stack_layers-1, c_map[2])\n",
    "\n",
    "    # Global average pooling and output\n",
    "    pool = AveragePooling(filter_shape=(8, 8), name='final_avg_pooling')(r3_2)\n",
    "    z = Dense(num_classes, init=normal(0.01))(pool)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go ahead and create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model dimensions\n",
    "image_height = 32\n",
    "image_width  = 32\n",
    "num_channels = 3 # RGB\n",
    "num_classes  = 10\n",
    "\n",
    "# Input variables denoting the features and label data\n",
    "input_var = C.input_variable((num_channels, image_height, image_width), name='features')\n",
    "label_var = C.input_variable((num_classes))\n",
    "\n",
    "# create model, and configure learning parameters - ResNet20\n",
    "resnet20_cifar10_model = create_cifar10_model(input_var, 3, num_classes)\n",
    "\n",
    "# loss and metric\n",
    "ce = cross_entropy_with_softmax(resnet20_cifar10_model, label_var)\n",
    "pe = classification_error(resnet20_cifar10_model, label_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it! We have now created a ResNet20 network architecture using the Microsoft Cognitive Toolkit API! Congratulations!\n",
    "\n",
    "If you want to see the structure of the model we have just created, the Cognitive Toolkit supports nice graphical visualiation using GraphViz. It can also give you a textual description of the structure of the model. You will need to have GraphViz installed (see http://www.graphviz.org/download/) and also the pydot_ng Python library (`pip install pydot_ng`).\n",
    "\n",
    "You won't be able to do this in Azure Notebooks, but if you have Jupter-Notebooks running locally, you can try the following code to generate the image.\n",
    "\n",
    "    import pydot_ng\n",
    "    \n",
    "    tmp_filename = \"images/ResNet20_Model.svg\"\n",
    "    graph_description = C.logging.graph.plot(resnet20_cifar10_model, tmp_filename)\n",
    "    \n",
    "    # you can also print out a textual description of the model using ...\n",
    "    #   print(graph_description)\n",
    "\n",
    "For those using Azure Notebooks, we will display the pre-generated image below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(\"images/ResNet20_Model.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to compare this SVG generated from the network model we created via the Microsoft Cognitive Toolkit with the ResNet20 network architecture image we looked at above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing our Dataset\n",
    "\n",
    "In this tutorial, we will be using the CIFAR-10 dataset (http://www.cs.toronto.edu/~kriz/cifar.html). \n",
    "CIFAR-10 is a popular dataset for image classification, collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. \n",
    "There are 50,000 training images and 10,000 test images. The 10 classes are: \n",
    "    \n",
    " * airplane, \n",
    " * automobile, \n",
    " * bird, \n",
    " * cat, \n",
    " * deer,\n",
    " * dog,\n",
    " * frog,\n",
    " * horse,\n",
    " * ship, and \n",
    " * truck.\n",
    "\n",
    "The CIFAR-10 dataset is not included in the Cognitive Toolkit distribution but can be easily downloaded and converted to supported \n",
    "format by the following code from Cognitive Toolkit directory `CNTK/Examples/Image/DataSets/CIFAR-10/`. We have copied this code into the `CIFAR-10` subdirectory of this lab.\n",
    "\n",
    "We need to use one of its helper scripts to download and process CIFAR-10. Normally we would run this using Python:\n",
    "\n",
    "    python install_cifar10.py\n",
    "\n",
    "This Python script will download, extract, and prepare the training and test data. You will see output like:\n",
    "\n",
    "    Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    Done.\n",
    "    Extracting files...\n",
    "    Done.\n",
    "    Preparing train set...\n",
    "    Done.\n",
    "    Preparing test set...\n",
    "    Done.\n",
    "    Writing train text file...\n",
    "    Done.\n",
    "    Writing test text file...\n",
    "    Done.\n",
    "    Converting train data to png images...\n",
    "    Done.\n",
    "    Converting test data to png images...\n",
    "    Done.\n",
    "    \n",
    "See https://github.com/Microsoft/CNTK/tree/master/Examples/Image/DataSets/CIFAR-10 for more information.\n",
    "\n",
    "But we can run it using the run magic command in this Jupyter-Notebook. We have a `copy_and_run.py` Python script, which copies these CIFAR-10 helper scripts to a subdirectory of `/home/nbuser/data` on Azure-Notebooks for speed and will then download, extract and prepare the training and test data.  Note that as we are using a custom script, our output will be *slightly* different to the reference output from `install_cifar10.py` above.\n",
    "\n",
    "This takes about 15-20 minutes to run. It tries to avoid unnecessary work, and so does not run if it finds its files already there -- although if you get into trouble, you can force it to run by passing a `-f` command argument.\n",
    "\n",
    "Unfortunately, the `/home/nbuser/data` area is quick but it is non-persistent, and will disappear at the end of your Jupyter-Notebook session. If this happens, when you run this script it will detect its files are missing and recreate as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i CIFAR-10/copy_and_run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Optional)\n",
    "\n",
    "The following code will train our ResNet model, using mini-batches.\n",
    "\n",
    "We would encourage you to experiment with training if possible in this lab. The data set is small. Having a conceptual understanding of what is involved in running training is really useful for consolidating your knowledge on what these learning algorithms are doing.\n",
    "\n",
    "However, Training can take some considerable time, even with the benefits of GPU acceleration, but especially so without. For those of you running only on a CPU (as you will be in Azure Notebooks), we have limited the number of rounds of training we do. Each round is called an *epoch*. Running for more epochs will usually result in a more accurate model, and so we are sacrificing accuracy for speed of progressing through the lab for CPU users.\n",
    "\n",
    "If you find the training too slow, feel free to skip the entire section below and proceed to the following section where we will leverage the fact that Microsoft makes available pre-trained models for download.  To skip it, set the variable `make_model` below to `false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Change this to FALSE if you want to skip training...\n",
    "#\n",
    "\n",
    "make_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in experimenting with training, you can run the code below and see how it works. Conceptually, the flow is as follows - we evaluate the error for the class prediction of a test image with a known label using our current set of weights, and then we use back-propagation to adjust our weights slightly to reduce this error. We do this for all the examples in our training set. This is one *epoch*. Then we repeat until either a fixed numer of epochs or, more likely, until our error has reduced to a suitable small number.\n",
    "\n",
    "In practice, we are splitting our test data in smaller chunks called minibatches. When all of the training data has been processed once across a number of minibatches, this is will be an *epoch*.\n",
    "\n",
    "Using minibatches has a number of advantages over single batch for training. Firstly, it uses less peak memory and therefore allows much larger training sets than might otherwise be feasible.  Secondly, it acts as a source of noise.  This small level of noise ensures that you escape being trapped in any local minima. Using minibatching does require that error information is accumulated across the entire epoch before doing backpropagation.\n",
    "\n",
    "![Training Workflow](images/cnn_training.gif \"Training Workflow\")\n",
    "\n",
    "If we have successfully set our default device to GPU, we will attempt to train for 50 epochs. This will take about 35 minutes with a GTX1080 Ti. If you want, you can reduce the `epochs = 50` line in the code to a smaller number, to train quicker, perhaps 5 or 1.\n",
    "\n",
    "Otherwise if using CPU we will attempt to train for only 1 epoch (to give a feeling for training, but without worrying about the accuracy of the resulting model, as we will use the pre-trained models).  \n",
    "\n",
    "If you have the patience and the time, we suggest also setting this to something like '5' and re-running from the top of the notebook. Compare the accuracy figures you get when evaluating your model at the end -- you should see that training for more epochs will usually give a more accurate model, up until the model converges..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = 60000  # number of training examples ...\n",
    "minibatch_size = 128 # examples per mini-batch for training\n",
    "\n",
    "if isUsingGPU:\n",
    "    epochs = 50\n",
    "else:\n",
    "    # these are the settings for CPU-only / Azure Notebooks\n",
    "    # if you have time/patience, consider changing the number of epochs from 1 to 5 and running again\n",
    "    epochs = 1\n",
    "    if make_model:\n",
    "        print(\"We're not using a GPU, so we're going to restrict our training for time reasons.\")\n",
    "        print(\"We'll be training for {} epoch with a training set of {} elements\".format(epochs, epoch_size))\n",
    "        \n",
    "model_dir = './out'\n",
    "if not model_dir:\n",
    "    model_dir = os.path.join(abs_path, \"Models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have configured our training, we are going to create a function to split our training dataset into smaller portions called *minibatches*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reader for both training and evaluation action.\n",
    "def create_image_minibatch_source(map_file, mean_file, train, total_number_of_samples):\n",
    "    if not os.path.exists(map_file) or not os.path.exists(mean_file):\n",
    "        raise RuntimeError(\"File '%s' or '%s' does not exist. Please run install_cifar10.py from DataSets/CIFAR-10 to fetch them\" %\n",
    "                           (map_file, mean_file))\n",
    "\n",
    "    # transformation pipeline for the features has jitter/crop only when training\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms += [\n",
    "            xforms.crop(crop_type='randomside', side_ratio=(0.8, 1.0), jitter_type='uniratio') # train uses jitter\n",
    "        ]\n",
    "    transforms += [\n",
    "        xforms.scale(width=image_width, height=image_height, channels=num_channels, interpolations='linear'),\n",
    "        xforms.mean(mean_file)\n",
    "    ]\n",
    "    # deserializer\n",
    "    return MinibatchSource(ImageDeserializer(map_file, StreamDefs(\n",
    "        features=StreamDef(field='image', transforms=transforms), # first column in map file is referred to as 'image'\n",
    "        labels=StreamDef(field='label', shape=num_classes))),     # and second as 'label'\n",
    "        randomize=train,\n",
    "        max_samples=total_number_of_samples,\n",
    "        multithreaded_deserializer=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're able to divide our data into minibatches, we'll create a train and evaluation function. For now, don't worry too much about understanding all of this code. It sets some training and learning parameters, creates a trainer (using the optimization algorithm *Stochastic Gradient Descent with Momentum*), and processes the training data in mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the network.\n",
    "def train_and_evaluate(reader_train, reader_test, epoch_size, max_epochs, profiler_dir=None,\n",
    "                       model_dir=None, log_dir=None, tensorboard_logdir=None, gen_heartbeat=False):\n",
    "\n",
    "    set_computation_network_trace_level(0)\n",
    "\n",
    "    # shared training parameters\n",
    "    l2_reg_weight = 0.0001\n",
    "\n",
    "    # Set learning parameters\n",
    "    lr_per_mb = [1.0]*80 + [0.1]*40 + [0.01]\n",
    "    lr_per_sample = [lr/minibatch_size for lr in lr_per_mb]\n",
    "    lr_schedule = learning_parameter_schedule_per_sample(lr_per_sample)\n",
    "    mm_schedule = momentum_schedule(0.9, minibatch_size)\n",
    "\n",
    "    # progress writers\n",
    "    progress_writers = [ProgressPrinter(tag='Training', log_to_file=log_dir, gen_heartbeat=gen_heartbeat)] \n",
    "\n",
    "    # trainer object\n",
    "    learner = momentum_sgd(resnet20_cifar10_model.parameters, lr_schedule, mm_schedule,\n",
    "                           l2_regularization_weight=l2_reg_weight)\n",
    "    trainer = Trainer(resnet20_cifar10_model, (ce, pe), learner, progress_writers)\n",
    "\n",
    "    # define mapping from reader streams to network inputs\n",
    "    input_map = {\n",
    "        input_var: reader_train.streams.features,\n",
    "        label_var: reader_train.streams.labels\n",
    "    }\n",
    "\n",
    "    # perform model training\n",
    "    print('[i] Performing training...')\n",
    "    for epoch in range(max_epochs): # loop over epochs\n",
    "        print('[i] Processing epoch {}'.format(epoch),flush=True)\n",
    "        sample_count = 0\n",
    "        pbar = tqdm(total=epoch_size, ascii=True)\n",
    "        while sample_count < epoch_size:  # loop over minibatches in the epoch\n",
    "            current_minibatch_size = min(minibatch_size, epoch_size-sample_count)\n",
    "            data = reader_train.next_minibatch(current_minibatch_size, input_map=input_map) # fetch minibatch.\n",
    "            trainer.train_minibatch(data)                  # update model with it\n",
    "            sample_count += current_minibatch_size         # count samples processed so far\n",
    "            pbar.update(current_minibatch_size)\n",
    "        pbar.close()\n",
    "\n",
    "        trainer.summarize_training_progress()\n",
    "\n",
    "        if model_dir:\n",
    "            resnet20_cifar10_model.save(os.path.join(model_dir, network_name + \"_{}.dnn\".format(epoch)))\n",
    "\n",
    "    # Evaluation parameters\n",
    "    test_epoch_size = 10000\n",
    "    test_minibatch_size = 16\n",
    "\n",
    "    # process minibatches and evaluate the model\n",
    "    metric_numer = 0\n",
    "    metric_denom = 0\n",
    "    sample_count = 0\n",
    "\n",
    "    print('[i] Evaluating model...', flush=True)\n",
    "    pbar = tqdm(total=test_epoch_size, ascii=True)\n",
    "    while sample_count < test_epoch_size:\n",
    "        current_minibatch_size = min(test_minibatch_size, test_epoch_size - sample_count)\n",
    "        # Fetch next test min batch.\n",
    "        data = reader_test.next_minibatch(current_minibatch_size, input_map=input_map)\n",
    "        # minibatch data to be trained with\n",
    "        metric_numer += trainer.test_minibatch(data) * current_minibatch_size\n",
    "        metric_denom += current_minibatch_size\n",
    "        # Keep track of the number of samples processed so far.\n",
    "        sample_count += data[label_var].num_samples\n",
    "        pbar.update(current_minibatch_size)\n",
    "    pbar.close()\n",
    "\n",
    "    print(\"\")\n",
    "    trainer.summarize_test_progress()\n",
    "    print(\"\")\n",
    "\n",
    "    return resnet20_cifar10_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Okay, with these functions, let's do the actual training itself.\n",
    "\n",
    "**WARNING:** this following code block takes significant time to run, especially if you are not using a GPU.  By default, we restrict training in this lab to a single epoch if on a CPU, but it still can take 15 minutes or so (per epoch on Azure Notebooks) to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_model:\n",
    "    model_file = 'LocallyTrained_ResNet_20_CIFAR10_CNTK.model'\n",
    "    print(\"make_model is true, so performing training...\")\n",
    "    print(\"Creating minibatch source for training...\", flush=True)\n",
    "    reader_train = create_image_minibatch_source(os.path.join(data_path, 'train_map.txt'), \n",
    "                                                 os.path.join(data_path, 'CIFAR-10_mean.xml'), \n",
    "                                                 True, \n",
    "                                                 total_number_of_samples=epochs * epoch_size)\n",
    "    print(\"Creating minibatch source for testing...\", flush=True)\n",
    "    reader_test = create_image_minibatch_source(os.path.join(data_path, 'test_map.txt'), \n",
    "                                                os.path.join(data_path, 'CIFAR-10_mean.xml'), \n",
    "                                                False, \n",
    "                                                total_number_of_samples=C.io.FULL_DATA_SWEEP)\n",
    "\n",
    "    startTime = time.time()\n",
    "    trained_model = train_and_evaluate(reader_train, reader_test, epoch_size, epochs)\n",
    "    endTime = time.time()\n",
    "    \n",
    "    print(\"The model took {:.4f} seconds to train\".format(endTime - startTime))\n",
    "    print(\"Saving the model as >>{}<<\".format(model_file))\n",
    "    \n",
    "    resnet20_cifar10_model.save(os.path.join(data_path, model_file))\n",
    "else:\n",
    "    model_file = 'ResNet_20_CIFAR10_CNTK.model'\n",
    "    print(\"make_model is false, so skipping training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see from the output above how well we did against the training set (`metric = `*xxxx*`%`) and against the test set (second `metrix = `*xxxx*`%` value). These metrics are a measure of what percentage of these sets we were correctly able to classify once we had trained our model. \n",
    "\n",
    "Did you notice one was more accurate than the other? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If not Training, Download Pre-Trained Model\n",
    "\n",
    "Microsoft maintains download links for all their pre-trained models at https://github.com/Microsoft/CNTK/blob/master/PretrainedModels/Image.md.  Some of the models are trained from scratch by the Cognitive Toolkit, and some others are converted from other toolkits such as Caffe. The models are categorized by their application domains.\n",
    "\n",
    "We're interested in the `ResNet20_CIFAR10_CNTK.model`.\n",
    "\n",
    "If you skipped training above, you must run this following block of code to download the model. If you have trained, it is still safe to run this if you so wish -- it will not replace your learned model with the downloaded version if `make_model` is set to `true`.  If `make_model` is set to `false`, it will download and use a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Load the appropriate urlretrieve function based on Python version\n",
    "try: \n",
    "    from urllib.request import urlretrieve \n",
    "except ImportError: \n",
    "    from urllib import urlretrieve\n",
    "    \n",
    "def ensure_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "        \n",
    "def download_model(model_root = data_path):\n",
    "    ensure_exists(model_root)\n",
    "    resnet_model_url = 'https://www.cntk.ai/Models/CNTK_Pretrained/ResNet20_CIFAR10_CNTK.model'\n",
    "    resnet_model_localfile = os.path.join(model_root, 'ResNet_20_CIFAR10_CNTK.model')\n",
    "    \n",
    "    try:\n",
    "        urlretrieve(resnet_model_url, resnet_model_localfile)\n",
    "        print('Download completed.')\n",
    "        return\n",
    "    except:\n",
    "        print('Failed to download, retrying.')\n",
    "  \n",
    "    return resnet_model_localfile\n",
    "\n",
    "if make_model != True:\n",
    "    print('Downloading pre-trained model. Note: this might take a while...')\n",
    "    startTime = time.time()\n",
    "    download_model()\n",
    "    endTime = time.time()\n",
    "    print('Downloading pre-trained model complete!')\n",
    "    print(\"The model took {:.4f} seconds to download\".format(endTime - startTime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function\n",
    "We next defined some evaluation functions to allow us to inspect the performance of our classifier.\n",
    "\n",
    "The first function will attempt to classify a single image, using the output from the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates a single image using the re-trained model\n",
    "def eval_single_image(loaded_model, image_path, width, height):\n",
    "    try:\n",
    "        img = PIL.Image.open(image_path).convert(\"RGBA\")    \n",
    "        if image_path.endswith(\"png\"):\n",
    "            temp = PIL.Image.new(\"RGBA\", img.size, (255, 255, 255))\n",
    "            temp.paste(img, img)\n",
    "            img = temp\n",
    "        resized = img.resize((height, width), PIL.Image.ANTIALIAS)\n",
    "        bgr_image = np.asarray(resized, dtype=np.float32)[..., [2, 1, 0]]\n",
    "        hwc_format = np.ascontiguousarray(np.rollaxis(bgr_image, 2))\n",
    "\n",
    "        # compute model output\n",
    "        arguments = {loaded_model.arguments[0]: [hwc_format]}\n",
    "        output = loaded_model.eval(arguments)\n",
    "\n",
    "        # return softmax probabilities\n",
    "        return C.softmax(output[0]).eval()\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not open (skipping file): {}\".format(image_path))\n",
    "        return ['None']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can evaluate the predicted class probabilities of a single image, we can extend this to perform some predictions on a test set.\n",
    "\n",
    "We're going to use the labels from the CIFAR-10, and use some Python to pretty-print the results in a Jupyter Notebook table. We're going to use a Pandas dataframe to display our results, so we need some code to setup helper routines for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This code is to allow us to inline our images for display in a table...\n",
    "#\n",
    "from IPython.display import HTML, display\n",
    "import urllib\n",
    "import pandas\n",
    "import PIL.Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "\n",
    "pandas.set_option('display.max_colwidth', -1)\n",
    "\n",
    "my_results = []\n",
    "\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def encode_image_as_base64(image_filepath):\n",
    "    image = PIL.Image.open(image_filepath)    \n",
    "    with BytesIO() as buffer:\n",
    "        image.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def map_image_file_to_inline_image(image_filepath):\n",
    "    return '<img src=\"data:image/jpeg;base64,{}\" width=64 height=64 alt=IMAGE />'.format(encode_image_as_base64(image_filepath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an evaluation function, to test a set of images against our provided model, by classifying each one and \n",
    "returning the list of predicted probabilities along with the ground truth, so let's create that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "# Evaluates an image set using the provided model\n",
    "def eval_test_images(loaded_model, test_map_file, width, height, max_images=-1, column_offset=0):\n",
    "    num_images = sum(1 for line in open(test_map_file))\n",
    "    if max_images > 0:\n",
    "        num_images = min(num_images, max_images)\n",
    "    if isUsingGPU is not False:\n",
    "        num_images = min(num_images, 300) #We will run through fewer images for test run on CPU...\n",
    "        \n",
    "    print(\"Evaluating model output node '{0}' for {1} images.\".format('prediction', num_images))\n",
    "\n",
    "    pred_count = 0\n",
    "    correct_count = 0\n",
    "    np.seterr(over='raise') # Set numpy to raise a FloatingPointError on any overflows...\n",
    "    with open(test_map_file, \"r\") as input_file:\n",
    "        for line in input_file:\n",
    "            tokens = line.rstrip().split('\\t')\n",
    "            img_file = tokens[0 + column_offset]\n",
    "            probs = eval_single_image(loaded_model, img_file, width, height)\n",
    "                \n",
    "            if probs[0]=='None':\n",
    "                print(\"Eval not possible: \", img_file)\n",
    "                continue\n",
    "            \n",
    "            precision = \"{:.4f}\"\n",
    "            \n",
    "            pred_count += 1\n",
    "            true_label = int(tokens[1 + column_offset])\n",
    "            predicted_label = np.argmax(probs)\n",
    "\n",
    "            #\n",
    "            # Create a table of results, showing the image, the ground truth label, our best prediction, and\n",
    "            # then the distribution of probabilities across all the classes\n",
    "            my_results.append({\"image_url\": img_file, \n",
    "                        \"truth\":       labels[true_label],\n",
    "                        \"prediction\":  labels[predicted_label],\n",
    "                        labels[0]:     precision.format(probs[0]),\n",
    "                        labels[1]:     precision.format(probs[1]),\n",
    "                        labels[2]:     precision.format(probs[2]),\n",
    "                        labels[3]:     precision.format(probs[3]),\n",
    "                        labels[4]:     precision.format(probs[4]),\n",
    "                        labels[5]:     precision.format(probs[5]),\n",
    "                        labels[6]:     precision.format(probs[6]),\n",
    "                        labels[7]:     precision.format(probs[7]),\n",
    "                        labels[8]:     precision.format(probs[8]),\n",
    "                        labels[9]:     precision.format(probs[9]),\n",
    "                   })\n",
    "\n",
    "            if predicted_label == true_label:\n",
    "                correct_count += 1\n",
    "\n",
    "            if pred_count % 100 == 0:\n",
    "                print(\"Processed {0} samples ({1:.2%} correct)\".format(pred_count, \n",
    "                                                                       (float(correct_count) / pred_count)))\n",
    "            if pred_count >= num_images:\n",
    "                break\n",
    "                    \n",
    "        # Let's format our results table nicely for Jupyter Notebook\n",
    "        # we'll use a Pandas DataFrame, and re-order the columns slightly so that the image,\n",
    "        # ground truth label and best prediction are displayed first...\n",
    "        df = pandas.DataFrame(my_results)\n",
    "        cols = list(df.columns.values)\n",
    "        cols.pop(cols.index('image_url'))\n",
    "        cols.pop(cols.index('truth'))\n",
    "        cols.pop(cols.index('prediction'))\n",
    "        df = df[['image_url'] + ['truth']+ ['prediction'] + cols]\n",
    "\n",
    "        # and finally, render it as a nice HTML table below\n",
    "        display(HTML(df.to_html(escape=False, formatters=dict(image_url=map_image_file_to_inline_image))))\n",
    "            \n",
    "    print (\"{0} of {1} prediction were correct\".format(correct_count, pred_count))\n",
    "    return correct_count, pred_count, (float(correct_count) / pred_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "\n",
    "Now that we have a trained model (either trained ourselves, or downloaded), let's get it loaded, and then we can evaluate the performance of the classifier against our test set. We're going to run some Jupyter-Notebook magic next to allow our tables format a bit nicer. This is just for presentation of the output of our classifier later, and you can safely ignore the contents of the next Markdown block, but please run it by selecting it and pressing `SHIFT` + `Return`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<!-- Some magic to ensure we can see our larger cell outputs -->\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:1000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how well our model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_model_file_path = os.path.join(data_path, model_file)\n",
    "if model_file is not None and os.path.exists(full_model_file_path):\n",
    "    print(\"Loading existing model from {}\".format(full_model_file_path))\n",
    "    trained_model = load_model(full_model_file_path)\n",
    "    print(\"{}\".format(trained_model))\n",
    "    eval_test_images(trained_model, os.path.join(data_path, \"test_map.txt\"), 32, 32, 100)\n",
    "else:\n",
    "    print(\"No pre-existing model file - either you need to perform training, or download a model...\")\n",
    "    if make_model != True:\n",
    "        print('Downloading pre-trained model. Note: this might take a while...')\n",
    "        startTime = time.time()\n",
    "        download_model()\n",
    "        endTime = time.time()\n",
    "        print('Downloading pre-trained model complete!')\n",
    "        print(\"The model took {:.4f} seconds to download\".format(endTime - startTime))\n",
    "        trained_model = load_model(full_model_file_path)\n",
    "        print(\"{}\".format(trained_model))\n",
    "        eval_test_images(trained_model, os.path.join(data_path, \"test_map.txt\"), 32, 32, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were doing the training yourself on CPU, an interesting exercise is to set `make_model` to `false` and re-run the code above to download the ready-made model from Microsoft. It has been trained on a much larger number of epochs, and so will have more accurately converged to a solution.  \n",
    "\n",
    "Compare your 1 or 5-epoch trained CPU model vs the pre-trained model. How does the accuracy differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this lab, we created a simple ResNet20 model, and used it to perform image classification on the CIFAR-10 dataset.\n",
    "\n",
    "Even on this limited dataset, it is possible to see how much more accurate the CNN-based approach is than traditional classifier using hand-crafted features - for example, Viola-Jones and the HOG algorithms.\n",
    "\n",
    "We've also seen that usually the more epochs we run training for, the more accurate our model gets - up to the point where it converges.\n",
    "\n",
    "In the next module, we will explore in more depth how to extend these CNN-based classifiers to make object detectors, and we will introduce the concepts of segmentation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
